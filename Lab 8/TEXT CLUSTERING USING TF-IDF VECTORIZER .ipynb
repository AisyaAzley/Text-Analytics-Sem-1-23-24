{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d97149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from tabulate import tabulate \n",
    "from collections import Counter \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2bee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\", \n",
    "           \"I enjoy hiking and camping in the mountains\", \n",
    "           \"I like to read books and watch movies\", \n",
    "           \"I prefer playing video games over sports\", \n",
    "           \"I love listening to music and going to concerts\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212cca8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cf75e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love playing football on the weekends',\n",
       " 'I enjoy hiking and camping in the mountains',\n",
       " 'I like to read books and watch movies',\n",
       " 'I prefer playing video games over sports',\n",
       " 'I love listening to music and going to concerts']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove punctuations in the documents\n",
    "def remove_punctuation(text):\n",
    "    # Initialize an empty string to store the result\n",
    "    punctuation_free = \"\"\n",
    "    \n",
    "    # Iterate over each character in the text\n",
    "    for i in text:\n",
    "        # Check if the character is not in the string.punctuation set\n",
    "        if i not in string.punctuation:\n",
    "            # If not, add the character to the result string\n",
    "            punctuation_free += i\n",
    "    return punctuation_free\n",
    "\n",
    "dataset = list(map(remove_punctuation, dataset))\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491e32bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love playing football on the weekends',\n",
       " 'i enjoy hiking and camping in the mountains',\n",
       " 'i like to read books and watch movies',\n",
       " 'i prefer playing video games over sports',\n",
       " 'i love listening to music and going to concerts']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to standardize the cases in the documents into lower case\n",
    "dataset = list(map(str.lower,dataset))\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa39c749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love playing football on the weekends',\n",
       " 'i enjoy hiking and camping in the mountains',\n",
       " 'i like to read books and watch movies',\n",
       " 'i prefer playing video games over sports',\n",
       " 'i love listening to music and going to concerts']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove numbers using re.sub ( ) in regular expression library\n",
    "\n",
    "#import regular expression library\n",
    "import re\n",
    "\n",
    "#function to remove digit (\\d) or hypens (-) from the documents with an empty string ''\n",
    "def remove_numbers(text):\n",
    "    return re.sub(\"[\\d-]\",'',text)\n",
    "\n",
    "#applying the remove_numbers function to the 'clean_lower' column and storing the result in a new column 'clean_number'\n",
    "dataset = list(map(remove_numbers,dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762143b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'playing', 'football', 'on', 'the', 'weekends'],\n",
       " ['i', 'enjoy', 'hiking', 'and', 'camping', 'in', 'the', 'mountains'],\n",
       " ['i', 'like', 'to', 'read', 'books', 'and', 'watch', 'movies'],\n",
       " ['i', 'prefer', 'playing', 'video', 'games', 'over', 'sports'],\n",
       " ['i', 'love', 'listening', 'to', 'music', 'and', 'going', 'to', 'concerts']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = [doc.split() for doc in dataset]\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739961dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Get the list of English stop words present in the library \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ebd6426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'playing', 'football', 'weekends'],\n",
       " ['enjoy', 'hiking', 'camping', 'mountains'],\n",
       " ['like', 'read', 'books', 'watch', 'movies'],\n",
       " ['prefer', 'playing', 'video', 'games', 'sports'],\n",
       " ['love', 'listening', 'music', 'going', 'concerts']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if i not in stopwords:\n",
    "            output.append(i)\n",
    "    return output\n",
    "\n",
    "#Applying the remove_stopwords function to the 'token_data' column and storing the result in a new column 'clean_xstopwords'\n",
    "tokenized_dataset = list(map(remove_stopwords,tokenized_dataset))\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfe8b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lov', 'play', 'footbal', 'weekend'],\n",
       " ['enjoy', 'hik', 'camp', 'mountain'],\n",
       " ['lik', 'read', 'book', 'watch', 'movy'],\n",
       " ['pref', 'play', 'video', 'gam', 'sport'],\n",
       " ['lov', 'list', 'mus', 'going', 'concert']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform word stemming using Lancaster Stemmer in nltk librarY\n",
    "\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "#defining the object for stemming\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = []\n",
    "    for word in text:\n",
    "        stemmed_word = lancaster_stemmer.stem(word)\n",
    "        stem_text.append(stemmed_word)\n",
    "    return stem_text\n",
    "\n",
    "#applying the stemming function to the 'clean_xstopwords' column and storing the result in a new column 'clean_stemmed' \n",
    "stemmed_dataset = list(map(stemming,tokenized_dataset))\n",
    "stemmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b0a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lov', 'play', 'footbal', 'weekend'], ['enjoy', 'hik', 'camp', 'mountain'], ['lik', 'read', 'book', 'watch', 'movy'], ['pref', 'play', 'video', 'gam', 'sport'], ['lov', 'list', 'mu', 'going', 'concert']]\n",
      "[['love', 'playing', 'football', 'weekend'], ['enjoy', 'hiking', 'camping', 'mountain'], ['like', 'read', 'book', 'watch', 'movie'], ['prefer', 'playing', 'video', 'game', 'sport'], ['love', 'listening', 'music', 'going', 'concert']]\n"
     ]
    }
   ],
   "source": [
    "## Perform word lemmatization using WordNetLemmatizer( ) in nltk library\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#importing the Lemmatizer function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = []\n",
    "    for word in text:\n",
    "        lemmatized_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        lemm_text.append(lemmatized_word)\n",
    "    return lemm_text\n",
    "\n",
    "# #applying the lemmatizer function to the 'clean_xstopwords' column and storing the result in a new column 'clean_lemmatized1'\n",
    "dataset_lemm_stem = list(map(lemmatizer,stemmed_dataset))\n",
    "\n",
    "# #applying the lemmatizer function to the 'clean_stemmed' column and storing the result in a new column 'clean_lemmatized2'\n",
    "dataset_lemm = list(map(lemmatizer,tokenized_dataset))\n",
    "\n",
    "print(dataset_lemm_stem)\n",
    "\n",
    "print(dataset_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4147925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset_lemm is a list of lists of lemmatized words\n",
    "# Concatenate the inner lists into single strings\n",
    "dataset_lemm_strings = [' '.join(sentence) for sentence in dataset_lemm]\n",
    "\n",
    "vectorizer = TfidfVectorizer() \n",
    "X = vectorizer.fit_transform(dataset_lemm_strings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516dabba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                              Predicted Cluster\n",
      "----------------------------------  -------------------\n",
      "love playing football weekend                         1\n",
      "enjoy hiking camping mountain                         1\n",
      "like read book watch movie                            0\n",
      "prefer playing video game sport                       1\n",
      "love listening music going concert                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      "Cluster 1:\n",
      " love\n",
      " playing\n",
      " football\n",
      " weekend\n",
      " enjoy\n",
      " camping\n",
      " mountain\n",
      " hiking\n",
      " sport\n",
      " music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 2  # Define the number of clusters \n",
    "km = KMeans(n_clusters=k) \n",
    "km.fit(X) \n",
    "\n",
    "# Predict the clusters for each document \n",
    "y_pred = km.predict(X) \n",
    " \n",
    "# Display the document and its predicted cluster in a table \n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset_lemm_strings, y_pred)]) \n",
    "print(tabulate(table_data, headers=\"firstrow\")) \n",
    " \n",
    "# Print top terms per cluster \n",
    "print(\"\\nTop terms per cluster:\") \n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "terms = vectorizer.get_feature_names_out() \n",
    "for i in range(k): \n",
    "    print(\"Cluster %d:\" % i) \n",
    "for ind in order_centroids[i, :10]: \n",
    "    print(' %s' % terms[ind]) \n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3bc9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f43f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
